{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Calibration\n",
    "\n",
    "This notebook implements Phase 1: baseline calibration of LLM responses to GSS ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from hivesight_calibration import (\n",
    "    GSSLoader, \n",
    "    PersonaGenerator, \n",
    "    LLMSurvey, \n",
    "    Calibrator,\n",
    "    OPINION_QUESTIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load GSS Data and Generate Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSS data\n",
    "loader = GSSLoader(data_dir=Path('../data'))\n",
    "gss = loader.load(years=[2022, 2024])\n",
    "\n",
    "# Focus on a single question for baseline: capital punishment\n",
    "question_id = 'cappun'\n",
    "question_text = loader.get_question_text(question_id)\n",
    "response_scale = loader.get_response_scale(question_id)\n",
    "\n",
    "print(f\"Question: {question_text}\")\n",
    "print(f\"Scale: {response_scale}\")\n",
    "\n",
    "# Filter to respondents who answered this question\n",
    "gss_valid = gss[gss[question_id].notna()].copy()\n",
    "print(f\"\\n{len(gss_valid):,} respondents with valid {question_id} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate personas from GSS respondents\n",
    "generator = PersonaGenerator()\n",
    "\n",
    "# Sample a subset for initial testing\n",
    "sample_size = 100  # Start small\n",
    "sample = gss_valid.sample(n=sample_size, random_state=42)\n",
    "\n",
    "personas = generator.from_dataframe(sample)\n",
    "print(f\"Generated {len(personas)} personas\")\n",
    "\n",
    "# Show example persona\n",
    "print(f\"\\nExample persona prompt:\")\n",
    "print(personas[0].to_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query LLM for Each Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM survey\n",
    "survey = LLMSurvey(model='gpt-4o-mini')\n",
    "\n",
    "async def run_survey(personas, question, scale):\n",
    "    \"\"\"Run survey for all personas.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for persona in tqdm(personas, desc=\"Querying LLM\"):\n",
    "        response = await survey.query(\n",
    "            persona=persona,\n",
    "            question=question,\n",
    "            response_type='likert',\n",
    "            scale=scale\n",
    "        )\n",
    "        results.append({\n",
    "            'llm_raw': response.raw_response,\n",
    "            'llm_parsed': response.parsed_response,\n",
    "            'tokens': response.tokens_total,\n",
    "            'cost': response.cost_usd\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the survey (uncomment when ready to spend API credits)\n",
    "# results = await run_survey(personas, question_text, response_scale)\n",
    "# print(f\"Total cost: ${sum(r['cost'] for r in results):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare LLM vs. Actual Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will be populated after running the LLM queries\n",
    "# For now, show the actual GSS distribution\n",
    "\n",
    "actual_dist = gss_valid[question_id].value_counts(normalize=True).sort_index()\n",
    "print(f\"GSS {question_id} distribution:\")\n",
    "print(actual_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit Calibration Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for calibration fitting\n",
    "# Will be implemented after LLM query results are collected\n",
    "\n",
    "# calibrator = Calibrator(n_categories=len(response_scale))\n",
    "# calibrator.fit(llm_responses, actual_responses, demographic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Calibration Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for evaluation\n",
    "# result = calibrator.evaluate(test_llm, test_actual, test_features)\n",
    "# print(f\"CRPS: {result.crps:.4f}\")\n",
    "# print(f\"Pinball losses: {result.pinball_losses}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
